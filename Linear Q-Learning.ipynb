{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"agent_linear.py\"\n",
    "\"\"\"Linear QL agent\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import framework\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "\n",
    "GAMMA = 0.5  # discounted factor\n",
    "TRAINING_EP = 0.5  # epsilon-greedy parameter for training\n",
    "TESTING_EP = 0.05  # epsilon-greedy parameter for testing\n",
    "NUM_RUNS = 10\n",
    "NUM_EPOCHS = 600\n",
    "NUM_EPIS_TRAIN = 25  # number of episodes for training at each epoch\n",
    "NUM_EPIS_TEST = 50  # number of episodes for testing\n",
    "ALPHA = 0.001  # learning rate for training\n",
    "\n",
    "ACTIONS = framework.get_actions()\n",
    "OBJECTS = framework.get_objects()\n",
    "NUM_ACTIONS = len(ACTIONS)\n",
    "NUM_OBJECTS = len(OBJECTS)\n",
    "\n",
    "\n",
    "def tuple2index(action_index, object_index):\n",
    "    \"\"\"Converts a tuple (a,b) to an index c\"\"\"\n",
    "    return action_index * NUM_OBJECTS + object_index\n",
    "\n",
    "\n",
    "def index2tuple(index):\n",
    "    \"\"\"Converts an index c to a tuple (a,b)\"\"\"\n",
    "    return index // NUM_OBJECTS, index % NUM_OBJECTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pragma: coderesponse template name=\"linear_epsilon_greedy\"\n",
    "def epsilon_greedy(state_vector, theta, epsilon):\n",
    "    \"\"\"Returns an action selected by an epsilon-greedy exploration policy\n",
    "\n",
    "    Args:\n",
    "        state_vector (np.ndarray): extracted vector representation\n",
    "        theta (np.ndarray): current weight matrix\n",
    "        epsilon (float): the probability of choosing a random command\n",
    "\n",
    "    Returns:\n",
    "        (int, int): the indices describing the action/object to take\n",
    "    \"\"\"\n",
    "    # TODO Your code here\n",
    "    action_index, object_index = None, None\n",
    "    return (action_index, object_index)\n",
    "# pragma: coderesponse end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pragma: coderesponse template\n",
    "def linear_q_learning(theta, current_state_vector, action_index, object_index,\n",
    "                      reward, next_state_vector, terminal):\n",
    "    \"\"\"Update theta for a given transition\n",
    "\n",
    "    Args:\n",
    "        theta (np.ndarray): current weight matrix\n",
    "        current_state_vector (np.ndarray): vector representation of current state\n",
    "        action_index (int): index of the current action\n",
    "        object_index (int): index of the current object\n",
    "        reward (float): the immediate reward the agent recieves from playing current command\n",
    "        next_state_vector (np.ndarray): vector representation of next state\n",
    "        terminal (bool): True if this epsiode is over\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # TODO Your code here\n",
    "    theta = None # TODO Your update here\n",
    "# pragma: coderesponse end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(for_training):\n",
    "    \"\"\" Runs one episode\n",
    "    If for training, update Q function\n",
    "    If for testing, computes and return cumulative discounted reward\n",
    "\n",
    "    Args:\n",
    "        for_training (bool): True if for training\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    epsilon = TRAINING_EP if for_training else TESTING_EP\n",
    "    epi_reward = None\n",
    "\n",
    "    # initialize for each episode\n",
    "    # TODO Your code here\n",
    "\n",
    "    (current_room_desc, current_quest_desc, terminal) = framework.newGame()\n",
    "    while not terminal:\n",
    "        # Choose next action and execute\n",
    "        current_state = current_room_desc + current_quest_desc\n",
    "        current_state_vector = utils.extract_bow_feature_vector(\n",
    "            current_state, dictionary)\n",
    "        # TODO Your code here\n",
    "\n",
    "        if for_training:\n",
    "            # update Q-function.\n",
    "            # TODO Your code here\n",
    "            pass\n",
    "\n",
    "        if not for_training:\n",
    "            # update reward\n",
    "            # TODO Your code here\n",
    "            pass\n",
    "\n",
    "        # prepare next step\n",
    "        # TODO Your code here\n",
    "\n",
    "    if not for_training:\n",
    "        return epi_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch():\n",
    "    \"\"\"Runs one epoch and returns reward averaged over test episodes\"\"\"\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(NUM_EPIS_TRAIN):\n",
    "        run_episode(for_training=True)\n",
    "\n",
    "    for _ in range(NUM_EPIS_TEST):\n",
    "        rewards.append(run_episode(for_training=False))\n",
    "\n",
    "    return np.mean(np.array(rewards))\n",
    "\n",
    "\n",
    "def run():\n",
    "    \"\"\"Returns array of test reward per epoch for one run\"\"\"\n",
    "    global theta\n",
    "    theta = np.zeros([action_dim, state_dim])\n",
    "\n",
    "    single_run_epoch_rewards_test = []\n",
    "    pbar = tqdm(range(NUM_EPOCHS), ncols=80)\n",
    "    for _ in pbar:\n",
    "        single_run_epoch_rewards_test.append(run_epoch())\n",
    "        pbar.set_description(\n",
    "            \"Avg reward: {:0.6f} | Ewma reward: {:0.6f}\".format(\n",
    "                np.mean(single_run_epoch_rewards_test),\n",
    "                utils.ewma(single_run_epoch_rewards_test)))\n",
    "    return single_run_epoch_rewards_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    state_texts = utils.load_data('game.tsv')\n",
    "    dictionary = utils.bag_of_words(state_texts)\n",
    "    state_dim = len(dictionary)\n",
    "    action_dim = NUM_ACTIONS * NUM_OBJECTS\n",
    "\n",
    "    # set up the game\n",
    "    framework.load_game_data()\n",
    "\n",
    "    epoch_rewards_test = []  # shape NUM_RUNS * NUM_EPOCHS\n",
    "\n",
    "    for _ in range(NUM_RUNS):\n",
    "        epoch_rewards_test.append(run())\n",
    "\n",
    "    epoch_rewards_test = np.array(epoch_rewards_test)\n",
    "\n",
    "    x = np.arange(NUM_EPOCHS)\n",
    "    fig, axis = plt.subplots()\n",
    "    axis.plot(x, np.mean(epoch_rewards_test,\n",
    "                         axis=0))  # plot reward per epoch averaged per run\n",
    "    axis.set_xlabel('Epochs')\n",
    "    axis.set_ylabel('reward')\n",
    "    axis.set_title(('Linear: nRuns=%d, Epilon=%.2f, Epi=%d, alpha=%.4f' %\n",
    "                    (NUM_RUNS, TRAINING_EP, NUM_EPIS_TRAIN, ALPHA)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
